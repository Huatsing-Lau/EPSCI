{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCI-Paper\n",
    "2023/9/10 11:20:41\n",
    "\n",
    "Code for the paper: \"Early Prognostication of Critical Patients with Spinal Cord Injury: A Machine Learning Study with 1485 Cases.\"\n",
    "\n",
    "By merging the MIMIC and eICU datasets, we developed a machine learning algorithm to predict the discharge destination (three-class classification) of ICU patients with spinal cord injury.\n",
    "\n",
    "We utilized a total of 7 feature selection algorithms in combination with 14 machine learning algorithms.\n",
    "\n",
    "Grid search and random search methodologies were employed to find the optimal hyperparameter combinations for the machine learning algorithms.\n",
    "\n",
    "We integrated several models with the best performance to create a comprehensive ensemble model with improved overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the development environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T04:20:56.879905Z",
     "start_time": "2023-09-03T04:20:56.871721Z"
    }
   },
   "outputs": [],
   "source": [
    "from ultils_for_ML_multiclass_classify import *\n",
    "from utils_SCI_data_process import write_lines_to_file\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T07:06:56.521601Z",
     "start_time": "2023-08-28T07:06:56.516837Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set global random seed\n",
    "from global_var import seed, random_state\n",
    "import random\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T07:11:53.279176Z",
     "start_time": "2023-08-28T07:11:53.188194Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dir_result = './SCI-Paper-results-discharge_location-first'#结果保存路径\n",
    "if not os.path.isdir(dir_result):\n",
    "    os.mkdir(dir_result)\n",
    "    \n",
    "from utils_SCI_data_process import SCI_data_process\n",
    "#　for new data:\n",
    "fname = './data/Version20201122/discharge_location/MIMIC-internal/imputed-MIMIC-first.csv'\n",
    "sheet_name = \"imputed-MIMIC-first\"\n",
    "Y_feature_name0 = \"discharge_location\"\n",
    "multi_class = True\n",
    "# Features to be deleted\n",
    "delete_columns = [\"drg_severity\",#外部测试集无此特征\n",
    "                  \"drg_mortality\",#外部测试集无此特征\n",
    "                  \"insurance\",#外部测试集无此特征\n",
    "                  \"marital_status\",#外部测试集无此特征\n",
    "                  \"los_emergency\",#外部测试集无此特征\n",
    "                  \"temperature\",#外部测试集无此特征\n",
    "                  \"Neutrophils\",#外部测试集无此特征\n",
    "                  #\"los_hospital\",\n",
    "                  \"los_icustays\",#手动删除，整理错误，无此特征\n",
    "                  #\"in.hospital.death\",#这是另外一个问题的应变量\n",
    "                 ]\n",
    "# category features\n",
    "dummy_columns = [#\"marital_status\",\n",
    "                 \"ethnicity\",\n",
    "                 \"gender\",\n",
    "                 \"careunit\",\n",
    "                 \"source.of.admission\",#Version20201122新增分类变量\n",
    "                ]\n",
    "\n",
    "#　Extract data\n",
    "YX_internal, X_feature_names_internal, Y_feature_name_internal = SCI_data_process(\n",
    "    fname,\n",
    "    sheet_name,\n",
    "    Y_feature_name0,\n",
    "    delete_columns,\n",
    "    dummy_columns,\n",
    "    multi_class = True,\n",
    ")\n",
    "\n",
    "## External Dataset\n",
    "# for new data:\n",
    "fname_external = './data/Version20201122/discharge_location/EICU-external/imputed/imputed-EICU-first.csv'\n",
    "sheet_name = \"imputed-EICU-first\"\n",
    "Y_feature_name = \"discharge_location\"\n",
    "# Features to be deleted\n",
    "delete_columns = [\"los_icustays\"]#手动删除，整理错误，无此特征]\n",
    "# category features\n",
    "dummy_columns = [#\"marital_status\",\n",
    "                 \"ethnicity\",\n",
    "                 \"gender\",\n",
    "                 \"careunit\",\n",
    "                 \"source.of.admission\",#Version20201122新增分类变量\n",
    "                ]\n",
    "\n",
    "YX_external, X_feature_names_external, Y_feature_name_external = SCI_data_process(fname_external,\n",
    "                                                                                  sheet_name,\n",
    "                                                                                  Y_feature_name0,\n",
    "                                                                                  delete_columns,\n",
    "                                                                                  dummy_columns,\n",
    "                                                                                  multi_class = True,\n",
    "                                                                                 )\n",
    "## Merge internal and external datasets\n",
    "import operator\n",
    "assert operator.eq(X_feature_names_internal,X_feature_names_external)\n",
    "assert operator.eq(Y_feature_name_internal,Y_feature_name_external)\n",
    "YX = pd.concat([YX_internal,YX_external],axis=0,ignore_index=True)\n",
    "\n",
    "X_feature_names, Y_feature_name = X_feature_names_internal, Y_feature_name_internal\n",
    "\n",
    "display(YX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution characteristics of raw data and data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T07:11:56.448762Z",
     "start_time": "2023-08-28T07:11:56.208490Z"
    }
   },
   "outputs": [],
   "source": [
    "## pie chart of Y\n",
    "one_hot_label = np.array(YX[Y_feature_name].values,dtype=int).tolist()\n",
    "label = [one_label.index(1) for one_label in one_hot_label] # 找到下标是1的位置\n",
    "label = pd.DataFrame(data=label,columns=['Y'])\n",
    "df = label.replace([0,1,2],Y_feature_name)['Y'].value_counts()\n",
    "\n",
    "f,ax = plt.subplots(dpi=100)\n",
    "df.plot.pie(figsize=(4, 4),autopct='%.2f',title=Y_feature_name0)#,index=['short','long']\n",
    "ax.set_ylabel('')\n",
    "fn = os.path.join(dir_result,'Internal Dataset Pie Chart of Dependent Variable.png')\n",
    "plt.savefig(fn)\n",
    "\n",
    "\n",
    "## Standardize and process to dataframe format to obtain X Y\n",
    "X = YX[X_feature_names]\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "X = sc.fit_transform(X)\n",
    "X = pd.DataFrame(data=X,columns=X_feature_names)\n",
    "Y = YX[Y_feature_name]\n",
    "\n",
    "# # split training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Save (non feature normalization processing) dataset partitioning results to a file \n",
    "# for human-machine countermeasures experiments\n",
    "fn = os.path.join(dir_result,'train_set.csv')\n",
    "YX.iloc[X_train.index].to_csv(fn)\n",
    "fn = os.path.join(dir_result,'test_set.csv')\n",
    "YX.iloc[X_test.index].to_csv(fn)\n",
    "\n",
    "processed_data={'X_train':X_train,\n",
    "                'Y_train':Y_train,\n",
    "                'X_test':X_test,\n",
    "                'Y_test':Y_test,\n",
    "                'target_names':['Dead','FMC','Home']\n",
    "               }\n",
    "display(X)\n",
    "\n",
    "n_repeats = 3 #P time\n",
    "n_splits = 5 #K fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-27T18:24:15.800204Z",
     "start_time": "2023-08-27T16:09:38.640040Z"
    }
   },
   "outputs": [],
   "source": [
    "# Building feature selection × Classification Algorithm Combination AUC Matrix\n",
    "\n",
    "feature_select_methods = ['MIC', 'RFE', 'EmbeddingLSVC', 'EmbeddingLR', 'EmbeddingTree', 'EmbeddingRF', 'mRMR']\n",
    "clf_names = ['Logistic', 'LDA', 'SVM', 'KNN', 'GaussianNB', 'Tree', 'ExtraTrees', 'RandomForest', 'Bagging', 'AdaBoost', 'GBDT', 'XGBoost', 'lightGBM', 'MLP']\n",
    "\n",
    "mean_aucs_matrix = dict([])\n",
    "aucs_testset_matrix = dict([])\n",
    "all_reslut_dict = dict([])\n",
    "best_clfs_dict = dict([])\n",
    "for k,method in enumerate(feature_select_methods):\n",
    "    print( 'Running {}...'.format(method) )\n",
    "    dir_result_method = os.path.join(dir_result,method)\n",
    "    if not os.path.isdir(dir_result_method):\n",
    "        os.mkdir(dir_result_method)\n",
    "\n",
    "    reslut_dict, aucs_RepeatedKFold_df, aucs_testset_df, aucs_external_df = run_algorithms(\n",
    "        data_internal=processed_data,\n",
    "        feature_select_method=method,\n",
    "        clf_names=clf_names,\n",
    "        kbest=15,#25\n",
    "        n_splits=n_splits,\n",
    "        n_repeats=n_repeats,\n",
    "        dir_result=dir_result_method)\n",
    "    \n",
    "    all_reslut_dict[method] = reslut_dict\n",
    "    \n",
    "    mean_aucs = dict([])\n",
    "    mean_aucs['micro'] = aucs_RepeatedKFold_df['micro'].mean(axis='columns')\n",
    "    mean_aucs['micro'] = pd.DataFrame(\n",
    "        data=mean_aucs['micro'].values[:,np.newaxis],\n",
    "        index=mean_aucs['micro'].index,\n",
    "        columns=[method])\n",
    "    mean_aucs['macro'] = aucs_RepeatedKFold_df['macro'].mean(axis='columns')\n",
    "    mean_aucs['macro'] = pd.DataFrame(\n",
    "        data=mean_aucs['macro'].values[:,np.newaxis],\n",
    "        index=mean_aucs['macro'].index,\n",
    "        columns=[method])\n",
    "    \n",
    "    if k==0:\n",
    "        mean_aucs_matrix['micro'] = mean_aucs['micro'].copy()\n",
    "        mean_aucs_matrix['macro'] = mean_aucs['macro'].copy()\n",
    "        aucs_testset_matrix['micro'] = aucs_testset_df['micro'].copy()\n",
    "        aucs_testset_matrix['macro'] = aucs_testset_df['macro'].copy()\n",
    "        aucs_external_matrix = aucs_external_df.copy() if aucs_external_df else None\n",
    "\n",
    "    else:\n",
    "        mean_aucs_matrix['micro'] = pd.concat([mean_aucs_matrix['micro'],mean_aucs['micro']],axis=1)\n",
    "        mean_aucs_matrix['macro'] = pd.concat([mean_aucs_matrix['macro'],mean_aucs['macro']],axis=1)\n",
    "        aucs_testset_matrix['micro'] = pd.concat([aucs_testset_matrix['micro'],aucs_testset_df['micro']],axis=1)\n",
    "        aucs_testset_matrix['macro'] = pd.concat([aucs_testset_matrix['macro'],aucs_testset_df['macro']],axis=1)\n",
    "        if aucs_external_df and aucs_external_matrix:\n",
    "            aucs_external_matrix = pd.concat([aucs_external_matrix,aucs_external_df],axis=1) \n",
    "        else:\n",
    "            ucs_external_matrix = None\n",
    "    best_clfs_dict[method] = dict( [ (clf_name, reslut_dict[clf_name]['clf'],) for clf_name in clf_names] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if each model has saved the corresponding test set report file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T07:07:35.253853Z",
     "start_time": "2023-08-28T07:07:35.235951Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if each model has saved the corresponding test set report file\n",
    "feature_select_methods = ['MIC','RFE','EmbeddingLSVC','EmbeddingLR','EmbeddingTree','EmbeddingRF','mRMR']\n",
    "clf_names = ['Logistic', 'LDA', 'SVM', 'KNN', 'GaussianNB', 'Tree', 'ExtraTrees', 'RandomForest', 'Bagging', 'AdaBoost', 'GBDT', 'XGBoost', 'lightGBM', 'MLP']\n",
    "# Load reports on the test set\n",
    "for k,sel_method in enumerate(feature_select_methods):\n",
    "    for clf_name in clf_names:\n",
    "        dir_result_method = os.path.join(dir_result,sel_method,clf_name)\n",
    "        fn = os.path.join(dir_result_method,'classification report of '+clf_name+' against test set.csv')\n",
    "        if not os.path.isfile(fn):\n",
    "            print(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T06:10:15.124977Z",
     "start_time": "2023-08-28T06:10:14.383887Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "all_vars = (\n",
    "    processed_data,\n",
    "    all_reslut_dict,\n",
    "    mean_aucs_matrix,\n",
    "    aucs_testset_matrix,\n",
    "    best_clfs_dict,\n",
    ")\n",
    "\n",
    "dir_vars = os.path.join(dir_result,'vars') \n",
    "if not os.path.exists( dir_vars ):\n",
    "    os.makedirs( dir_vars )\n",
    "\n",
    "all_vars_filename = os.path.join(dir_vars,'all_vars.pkl')\n",
    "# Save the packaged variables to a file\n",
    "with open(all_vars_filename, \"wb\") as f:\n",
    "    pickle.dump(all_vars, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If there were previous training results, load the result data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T07:15:17.620660Z",
     "start_time": "2023-08-28T07:15:16.954804Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import pickle\n",
    "    dir_vars = os.path.join(dir_result,'vars') \n",
    "    all_vars_filename = os.path.join(dir_vars,'all_vars.pickle')\n",
    "    with open(all_vars_filename, 'rb') as f:  \n",
    "        processed_data, all_reslut_dict, mean_aucs_matrix, aucs_testset_matrix, best_clfs_dict = pickle.loads(f.read())\n",
    "    feature_select_methods = ['MIC','RFE','EmbeddingLSVC','EmbeddingLR','EmbeddingTree','EmbeddingRF','mRMR']\n",
    "    clf_names = ['Logistic', 'LDA', 'SVM', 'KNN', 'GaussianNB', 'Tree', 'ExtraTrees', 'RandomForest', 'Bagging', 'AdaBoost', 'GBDT', 'XGBoost', 'lightGBM', 'MLP']\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T07:15:19.103632Z",
     "start_time": "2023-08-28T07:15:18.945599Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dict = dict([(sel,dict([])) for sel in feature_select_methods])\n",
    "for k,sel_method in enumerate(feature_select_methods):\n",
    "    for clf_name in clf_names:\n",
    "        dir_result_method = os.path.join(dir_result,sel_method,clf_name)\n",
    "        fn = os.path.join(dir_result_method,'classification report of '+clf_name+' against test set.csv')\n",
    "        df = pd.read_csv(fn,index_col=0)\n",
    "        df_dict[sel_method][clf_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T07:15:21.175912Z",
     "start_time": "2023-08-28T07:15:21.034428Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load reports on the test set\n",
    "Dead_recall_matrix = pd.DataFrame(columns=clf_names, index=feature_select_methods)\n",
    "acc_matrix = pd.DataFrame(columns=clf_names, index=feature_select_methods)\n",
    "for k,sel_method in enumerate(feature_select_methods):\n",
    "    for clf_name in clf_names:\n",
    "        Dead_recall_matrix.loc[sel_method,clf_name] = all_reslut_dict[sel_method][clf_name]['metric']['test']['report'].loc['Dead','recall']\n",
    "        acc_matrix.loc[sel_method,clf_name]  = all_reslut_dict[sel_method][clf_name]['metric']['test']['report'].loc['accuracy','recall']\n",
    "acc_matrix = acc_matrix.apply(pd.to_numeric, errors = 'ignore')\n",
    "Dead_recall_matrix = Dead_recall_matrix.apply(pd.to_numeric, errors = 'ignore')\n",
    "print(\"Dead recall on the test set::\")\n",
    "display(Dead_recall_matrix)\n",
    "print(\"Acuracy on the test set::\")\n",
    "display(acc_matrix)\n",
    "\n",
    "roc_auc_matrix = {\n",
    "    'micro':pd.DataFrame(columns=clf_names, index=feature_select_methods), \n",
    "    'macro':pd.DataFrame(columns=clf_names, index=feature_select_methods)}\n",
    "for k,sel_method in enumerate(feature_select_methods):\n",
    "    for clf_name in clf_names:\n",
    "        roc_auc_matrix['micro'].loc[sel_method,clf_name] = all_reslut_dict[sel_method][clf_name]['metric']['test']['roc_auc']['micro']\n",
    "        roc_auc_matrix['macro'].loc[sel_method,clf_name] = all_reslut_dict[sel_method][clf_name]['metric']['test']['roc_auc']['macro']\n",
    "print(\"Micro auc on the test set::\")\n",
    "display(roc_auc_matrix['micro'])\n",
    "print(\"Macro auc on the test set:\")\n",
    "display(roc_auc_matrix['macro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T07:15:26.324547Z",
     "start_time": "2023-08-28T07:15:25.326189Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dead Recall on Test Set\n",
    "plot_mean_aucs_matirx(Dead_recall_matrix, fmt='.3g',\n",
    "                      title='Dead-Recall on Test-Set',\n",
    "                      fn=os.path.join(dir_result,'Dead-Recall on Test-Set.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T07:15:30.564822Z",
     "start_time": "2023-08-28T07:15:26.327152Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# AUC matrix for cross validation\n",
    "plot_mean_aucs_matirx(mean_aucs_matrix['micro'].T, fmt='.3g',\n",
    "                      title='Mean Value of micro-average AUC of Cross Validation',\n",
    "                      fn=os.path.join(dir_result,'Mean Value of micro-average AUC of Cross Validation.png'))\n",
    "plot_mean_aucs_matirx(mean_aucs_matrix['macro'].T, fmt='.3g',\n",
    "                      title='Mean Value of macro-average AUC of Cross Validation',\n",
    "                      fn=os.path.join(dir_result,'Mean Value of macro-average AUC of Cross Validation.png'))\n",
    "# AUC matrix of internal test set\n",
    "plot_mean_aucs_matirx(aucs_testset_matrix['micro'].T, fmt='.3g',\n",
    "                      title='micro-average AUC on Test Set',\n",
    "                      fn=os.path.join(dir_result,'micro-average AUC on Test-Set.png'))\n",
    "plot_mean_aucs_matirx(aucs_testset_matrix['macro'].T, fmt='.3g',\n",
    "                      title='macro-average AUC on Test Set',\n",
    "                      fn=os.path.join(dir_result,'macro-average AUC on Test-Set.png'))\n",
    "# AUC matrix of external test set\n",
    "if aucs_external_matrix:\n",
    "    plot_mean_aucs_matirx(aucs_external_matrix.T, fmt='.3g',\n",
    "                          title='AUC Matrix on External Data-set',\n",
    "                          fn=os.path.join(dir_result,'AUC Matrix on External Data-set.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the best classifiers and integrate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T08:55:57.811183Z",
     "start_time": "2023-08-28T08:55:57.793747Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_max_value_index(df):\n",
    "    \"\"\"Find the row index and column index where the maximum value is located in the given DataFrame\"\"\"\n",
    "    max_val = df.values.max()  # 找到 DataFrame 中的最大值\n",
    "    max_val_idx = df[df == max_val].stack().index[0]  # 找到最大值所在的位置\n",
    "    row_idx, col_idx = max_val_idx  # 解析出行索引和列索引\n",
    "    return (row_idx, col_idx)\n",
    "\n",
    "def find_topk_indices(df, k):\n",
    "    # 使用 stack 将 DataFrame 转换为 Series，并按值降序排序\n",
    "    sorted_series = df.stack().sort_values(ascending=False)\n",
    "\n",
    "    # 取前 k 个元素的索引\n",
    "    topk_indices = sorted_series.head(k).index\n",
    "\n",
    "    # 将索引分解为 index 和 column\n",
    "    topk_index = [idx[0] for idx in topk_indices]\n",
    "    topk_column = [idx[1] for idx in topk_indices]\n",
    "\n",
    "    position = [(index,col) for index, col in zip(topk_index, topk_column)]\n",
    "    return position\n",
    "\n",
    "def find_indices_greater_than(df, thres:float):\n",
    "    indices = df.stack()[df.stack()>thres]\n",
    "    position = indices.index.tolist()\n",
    "    return position\n",
    "\n",
    "def get_topk_candidate(candidates,topk:int=1):\n",
    "    \"\"\"\n",
    "    返回取值最大的候选算法。\n",
    "    \"\"\"\n",
    "    candidates.sort(reverse=True,key=lambda x: x[1])\n",
    "    return [candidates[k][0] for k in range(topk)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T09:06:03.454614Z",
     "start_time": "2023-08-28T09:06:03.428962Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# top_models_dict = dict([])\n",
    "# top_models_dict['top recall'] = {'name': find_max_value_index(df=Dead_recall_matrix)}\n",
    "# top_models_dict['top micro auc'] = {'name': find_max_value_index(df=roc_auc_matrix['micro'])}\n",
    "# top_models_dict['top macro auc'] = {'name': find_max_value_index(df=roc_auc_matrix['macro'])}\n",
    "# if top_models_dict['top micro auc']['name'] == top_models_dict['top macro auc']['name']:\n",
    "#     # 由于top micro和top macro是同一个模型，因此合并一下。\n",
    "#     top_models_dict.pop('top micro auc')\n",
    "#     top_models_dict['top auc'] = top_models_dict.pop('top macro auc')\n",
    "# # # 手工指定和上一次运行结果一样的算法\n",
    "# # top_models_dict['top auc'] = {'name': ('EmbeddingLSVC','lightGBM')}\n",
    "# top_models_dict\n",
    "\n",
    "\n",
    "top_models_dict = dict([])\n",
    "k = 10\n",
    "position1a = find_indices_greater_than(Dead_recall_matrix, Dead_recall_matrix.values.flatten().max()*0.80)\n",
    "position1b = find_topk_indices(Dead_recall_matrix, k)\n",
    "position1 = set(position1a).union( set(position1b) )\n",
    "\n",
    "position2a = find_indices_greater_than(roc_auc_matrix['micro'], roc_auc_matrix['micro'].values.flatten().max()*0.80)\n",
    "position2b = find_topk_indices(roc_auc_matrix['micro'], k)\n",
    "position2 = set(position2a).union( set(position2b) )\n",
    "\n",
    "position = list(set(position1).intersection(set(position2)))\n",
    "print(position)\n",
    "# top recall\n",
    "Dead_Recall_candidates = [((index,col),Dead_recall_matrix.loc[index,col]) for (index,col) in position]\n",
    "Dead_Recall_Best_candidate = get_topk_candidate(Dead_Recall_candidates,1)[0]\n",
    "top_models_dict['top recall'] = {'name':Dead_Recall_Best_candidate}\n",
    "# top_models_dict['top recall'] = {'name':('EmbeddingRF','lightGBM')}\n",
    "\n",
    "# top auc\n",
    "AUC_candidates = [((index,col),roc_auc_matrix['micro'].loc[index,col]) for (index,col) in position]\n",
    "AUC_Best_candidate = get_topk_candidate(AUC_candidates,1)[0]\n",
    "top_models_dict['top auc'] = {'name': AUC_Best_candidate}\n",
    "top_models_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T09:16:39.759094Z",
     "start_time": "2023-08-28T09:16:05.666963Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ultils_for_ML_multiclass_classify import get_algorithm_result\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "        self.n_features_in_ = len(attribute_names)\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "\n",
    "\n",
    "pipeline = np.zeros([3]).tolist()\n",
    "for k, top_model in top_models_dict.items():\n",
    "    feature_selector_name, clf_name = top_model['name']\n",
    "    top_models_dict[k]['clf'] = copy.deepcopy(best_clfs_dict[ feature_selector_name ][ clf_name ])\n",
    "    selected_features = all_reslut_dict[feature_selector_name]['selected_features_names'] #特征名称列表\n",
    "    selector = DataFrameSelector(selected_features)\n",
    "\n",
    "    top_models_dict[k]['pipeline'] = (clf_name+'_'+feature_selector_name, Pipeline( [('selector',selector),('clf',top_models_dict[k]['clf'])] ))\n",
    "    \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipelines = [top_models_dict[k]['pipeline'] for k in top_models_dict.keys()]\n",
    "\n",
    "# from sklearn.ensemble import StackingClassifier\n",
    "# Stacking_clf = StackingClassifier(estimators=pipelines)#, final_estimator=LogisticRegression)\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "ensem_clf = VotingClassifier(estimators=pipelines, voting='soft', weights=[3.0, 1.0])\n",
    "\n",
    "# 获取特征选择后的数据\n",
    "X_train = processed_data['X_train']#.values#dataframe\n",
    "X_test = processed_data['X_test']#.values#dataframe\n",
    "Y_train = onehot2label( processed_data['Y_train'].values )\n",
    "Y_test = onehot2label( processed_data['Y_test'].values )\n",
    "X_external,Y_external = None,None\n",
    "target_names = ['Dead','FMC','Home']\n",
    "# 运行交叉验证、内部数据集测试、外部数据集测试\n",
    "ret_ensem_clf = get_algorithm_result(X_train,Y_train,\n",
    "                                     X_test,Y_test,\n",
    "                                     X_external,Y_external,\n",
    "                                     target_names,\n",
    "                                     n_splits,n_repeats,\n",
    "                                     classifier=ensem_clf,#Stacking_clf,#ensem_clf, \n",
    "                                     clf_name='Ensemble Model',#'StackingClassifier',#'VotingClassifier',\n",
    "                                     dir_result=dir_result)\n",
    "\n",
    "ensem_clf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between Ensemble Model and Best Single Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCA Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T09:17:16.451199Z",
     "start_time": "2023-08-28T09:17:13.111043Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_decision_curve(y_true, y_pred_prob, class_names, pos_frac=None):\n",
    "    \"\"\"\n",
    "    Comparison of Integrated Model and Best Single Model to Generate Clinical Decision Curve for Triad Classification Predictive Model Test Set\n",
    "    : param y_ True: true label of the test set\n",
    "\n",
    "    : param y_ Pred_ Prob: Prediction probability of each category in the test set\n",
    "\n",
    "    : param class_ Names: Label Category Name\n",
    "    \"\"\"\n",
    "    n = len(y_true)\n",
    "\n",
    "    if pos_frac is None:\n",
    "        # Calculate the proportion of positive samples for each category\n",
    "        pos_frac = [np.mean(y_true == i) for i in range(len(class_names))]\n",
    "\n",
    "    # Calculate thresholds and benefits for each CE category\n",
    "    thresh = np.linspace(0, 1, 100)\n",
    "    ce = []\n",
    "    ap = []\n",
    "    an = []\n",
    "    for t in thresh:\n",
    "        ce_t = 0\n",
    "        ap_t = 0\n",
    "        an_t = 0\n",
    "        for k in range(len(class_names)):\n",
    "            y_k = (y_true == k)\n",
    "            pred_k = (y_pred_prob[:, k] > t).astype(int)\n",
    "            tp = np.dot(y_k, pred_k)\n",
    "            fp = np.sum((pred_k == 1) & (y_k == 0))\n",
    "            fn = np.sum((pred_k == 0) & (y_k == 1))\n",
    "#             ce_t += 1/len(class_names) * 1/n*(tp-fp*t/(1-t))\n",
    "            ce_t += pos_frac[k] * 1/n*(tp-fp*t/(1-t))\n",
    "            pr = (fn+fp)/n\n",
    "#             ap_t += 1/len(class_names)  * (pr-(1-pr)*t/(1-t))\n",
    "            ap_t += pos_frac[k] * (pr-(1-pr)*t/(1-t))\n",
    "            an_t += 0.0\n",
    "        ce.append(ce_t)\n",
    "        ap.append(ap_t)\n",
    "        an.append(an_t)\n",
    "\n",
    "    return thresh, ce, ap, an\n",
    "\n",
    "\n",
    "# y_pred_prob = ensem_clf.predict_proba(X_train)\n",
    "# plot_decision_curve(Y_train, y_pred_prob, class_names=[0,1,2])\n",
    "y_pred_prob = ensem_clf.predict_proba(X_test)\n",
    "thresh, ce, ap, an = get_decision_curve(Y_test, y_pred_prob, class_names=[0,1,2])\n",
    "\n",
    "pipelines[0][1].fit(X_train,Y_train)\n",
    "y_pred_prob = pipelines[0][1].predict_proba(X_test)\n",
    "thresh_m0, ce_m0, ap_m0, an_m0 = get_decision_curve(Y_test, y_pred_prob, class_names=[0,1,2])\n",
    "\n",
    "pipelines[1][1].fit(X_train,Y_train)\n",
    "y_pred_prob = pipelines[1][1].predict_proba(X_test)\n",
    "thresh_m1, ce_m1, ap_m1, an_m1 = get_decision_curve(Y_test, y_pred_prob, class_names=[0,1,2])\n",
    "\n",
    "# Draw DCA\n",
    "fig, ax = plt.subplots(1,1,dpi=300)\n",
    "ax.plot(thresh, ce, label='overall DCA, Ensemble Model')\n",
    "ax.plot(thresh, ce_m0, label='overall DCA, Dead Recall Best Model')\n",
    "ax.plot(thresh, ce_m1, label='overall DCA, AUC Best Model')\n",
    "ax.plot(thresh, ap, ':', label='overall all positive')\n",
    "ax.plot(thresh, an, ':', label='overall all negative')\n",
    "ax.set_xlabel('thresh')\n",
    "ax.set_ylabel('CE')\n",
    "ax.set_title('Clinical Decision Curve')\n",
    "ax.legend()\n",
    "ax.set_ylim(-0.05,max([max(ce),max(ap),max(an)]))\n",
    "plt.savefig( os.path.join(dir_result,'Ensemble Model','集成模型和最佳单模型在测试集的DCA曲线对比.png'), dpi=300 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### delong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T09:17:20.415614Z",
     "start_time": "2023-08-28T09:17:20.246039Z"
    }
   },
   "outputs": [],
   "source": [
    "# import delong\n",
    "y_prob_0 = pipelines[0][1].predict_proba(X_test)\n",
    "y_prob_1 = pipelines[1][1].predict_proba(X_test)\n",
    "y_prob_ens = ensem_clf.predict_proba(X_test)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Convert to one hot encoding through OneHotEncoder, data_ Onehot is the converted result\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_true = encoder.fit_transform(np.array(Y_test).reshape(-1, 1))\n",
    "\n",
    "df_delong = pd.DataFrame(index=['Dead Recall Best Model', 'AUC Best Model', 'Ensemble Model'], columns=['Dead Recall Best Model', 'AUC Best Model', 'Ensemble Model'])\n",
    "for name0,y0 in zip(['Dead Recall Best Model', 'AUC Best Model', 'Ensemble Model'],[y_prob_0, y_prob_1, y_prob_ens]):\n",
    "    for name1, y1 in zip(['Dead Recall Best Model', 'AUC Best Model', 'Ensemble Model'],[y_prob_0, y_prob_1, y_prob_ens]):\n",
    "        p_value, auc1, auc2, auc_diff = delong_test(y_true, y0, y1)\n",
    "        df_delong.loc[name0,name1] = p_value\n",
    "        \n",
    "df_delong.to_csv(os.path.join(dir_result,'Ensemble Model','集成模型和最佳单模型在测试集的delong检验p值.csv'))\n",
    "display(df_delong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between Ensemble Model and other single models\n",
    "\n",
    "1. The sensitivity of the Dead category is higher than other single models (AUCs) of the integrated model.\n",
    "\n",
    "2. AUC is higher than the Dead sensitivity of a single model in an integrated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T09:17:23.802226Z",
     "start_time": "2023-08-28T09:17:23.792951Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_elements_greater_than_value(df, value):\n",
    "    \"\"\"\n",
    "    Using NumPy's vectorization operation to find elements greater than a specific value, \n",
    "    returns obtaining row and column indexes that meet the conditions\n",
    "    \"\"\"\n",
    "    idxs = np.where(df.values > value)\n",
    "    elms = [(df.index[row], df.columns[col]) for row, col in zip(idxs[0],idxs[1])]\n",
    "    return elms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Single Models (AUC) with higher Dead Recall than Ensemble Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T09:17:26.953238Z",
     "start_time": "2023-08-28T09:17:26.934998Z"
    }
   },
   "outputs": [],
   "source": [
    "lines= []\n",
    "line = f\"The Dead sensitivity of the ensemble model in the test set:, {ret_ensem_clf['test']['report'].loc['Dead','recall']}\"\n",
    "print( line )\n",
    "lines.append(line)\n",
    "\n",
    "line = 'roc auc of Integration model on test set:' \n",
    "print( line )\n",
    "lines.append(line)\n",
    "\n",
    "line = str(ret_ensem_clf['test']['report'].loc[:,'roc-auc'])\n",
    "print( line )\n",
    "lines.append(line)\n",
    "\n",
    "#Previously obtained the Dead sensitivity of all single models Dead_ Recall_ Matrix\n",
    "#The sensitivity of the Dead category is higher than that of other single models (AUCs) in the integrated model\n",
    "line = 'Other single models (AUC) with higher sensitivity than the Dead category of ensemble models:'\n",
    "print(line)\n",
    "lines.append(line)\n",
    "elms = find_elements_greater_than_value(Dead_recall_matrix, ret_ensem_clf['test']['report'].loc['Dead','recall'])\n",
    "for idx, col in elms:\n",
    "    line = f\"{idx}-{col}: Dead-recall: {Dead_recall_matrix.loc[idx,col]:.3f}, micro-AUC: {roc_auc_matrix['micro'].loc[idx,col]:.3f}, macro-AUC: {roc_auc_matrix['macro'].loc[idx,col]:.3f}\"\n",
    "    print( line )\n",
    "    lines.append(line)\n",
    "    \n",
    "file_path=os.path.join(dir_result,'Ensemble Model','集成模型和Dead灵敏度高于集成模型的单模型的指标对比.xlsx')\n",
    "outputfile=pd.ExcelWriter(file_path)\n",
    "pd.DataFrame(lines).to_excel(outputfile,sheet_name='AUC对比')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### delong test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T09:17:32.873277Z",
     "start_time": "2023-08-28T09:17:32.740260Z"
    }
   },
   "outputs": [],
   "source": [
    "# 执行delong检验，观察单模型和集成模型的AUC曲线差异是否有统计学意义？\n",
    "\n",
    "single_models_dict = dict([])\n",
    "for feature_selector_name, clf_name in elms:\n",
    "    single_models_dict[clf_name+'_'+feature_selector_name] = dict([])\n",
    "    selected_features = all_reslut_dict[feature_selector_name]['selected_features_names'] #特征名称列表\n",
    "    selector = DataFrameSelector(selected_features)\n",
    "    single_models_dict[clf_name+'_'+feature_selector_name]['selected_features'] = selected_features\n",
    "    single_models_dict[clf_name+'_'+feature_selector_name]['clf'] = copy.deepcopy(best_clfs_dict[ feature_selector_name ][ clf_name ])\n",
    "    single_models_dict[clf_name+'_'+feature_selector_name]['pipeline'] = (clf_name+'_'+feature_selector_name, Pipeline( [('selector',selector),('clf',copy.deepcopy(best_clfs_dict[ feature_selector_name ][ clf_name ]))] ))\n",
    "\n",
    "# 单模型预测概率\n",
    "single_models_y_prob = dict([])\n",
    "\n",
    "for pipeline_name, val in single_models_dict.items():\n",
    "    single_models_y_prob[pipeline_name] =  val['pipeline'][1].predict_proba(X_test)\n",
    "# 集成模型预测概率\n",
    "y_prob_ens = ensem_clf.predict_proba(X_test)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# 通过 OneHotEncoder 转换成 one-hot 编码，data_onehot 为转换后的结果\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_true = encoder.fit_transform(np.array(Y_test).reshape(-1, 1))\n",
    "\n",
    "names = list(single_models_dict.keys())+['Ensemble Model']\n",
    "y_probs = list(single_models_y_prob.values())+[y_prob_ens]\n",
    "df_delong = pd.DataFrame(index=names, columns=names)\n",
    "for name0,y0 in zip(names, y_probs):\n",
    "    for name1, y1 in zip(names,y_probs):\n",
    "        p_value, auc1, auc2, auc_diff = delong_test(y_true, y0, y1)\n",
    "        df_delong.loc[name0,name1] = p_value\n",
    "display(df_delong)\n",
    "\n",
    "# df_delong.to_csv(lines, file_path=os.path.join(dir_result,'VotingClassifier','集成模型和Dead灵敏度高于集成模型的单模型的delong检验.csv'))\n",
    "df_delong.to_excel(outputfile,sheet_name='delong检验')  \n",
    "outputfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dead Recall of Single Models with AUC Higher than Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T09:17:39.927337Z",
     "start_time": "2023-08-28T09:17:39.908877Z"
    }
   },
   "outputs": [],
   "source": [
    "lines = []\n",
    "line = f\"Ensemble model micro AUC in the test set:, {ret_ensem_clf['test']['report'].loc['micro avg']['roc-auc']}\"\n",
    "lines.append(line)\n",
    "print( line )\n",
    "\n",
    "line = f\"Ensemble model Dead Recall in the test set:', {ret_ensem_clf['test']['report'].loc['Dead','recall']}\"\n",
    "lines.append(line)\n",
    "print( line )\n",
    "\n",
    "\n",
    "# Previously obtained the Dead Recall of all single models: Dead_ Recall_ Matrix\n",
    "elms = find_elements_greater_than_value(roc_auc_matrix['micro'], ret_ensem_clf['test']['report'].loc['micro avg']['roc-auc'])\n",
    "for idx, col in elms:\n",
    "    line = f\"{idx}-{col}: Dead-recall: {Dead_recall_matrix.loc[idx,col]:.3f}, micro-AUC: {roc_auc_matrix['micro'].loc[idx,col]:.3f}, macro-AUC: {roc_auc_matrix['macro'].loc[idx,col]:.3f}\"\n",
    "    print( line )\n",
    "    lines.append(line)\n",
    "write_lines_to_file(lines, file_path=os.path.join(dir_result,'Ensemble Model','集成模型和AUC高于集成模型的单模型的Dead灵敏度对比.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance of Ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T07:15:02.781992Z",
     "start_time": "2023-09-03T07:15:02.718031Z"
    }
   },
   "outputs": [],
   "source": [
    "################### Calculate Permutation Importance ##################\n",
    "ensem_clf = ensem_clf\n",
    "ensem_clf_features = []\n",
    "for pipeline in ensem_clf.estimators:\n",
    "    ensem_clf_features += pipeline[-1][0].attribute_names\n",
    "ensem_clf_features = sorted(set(ensem_clf_features))\n",
    "\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "perm = PermutationImportance(ensem_clf, random_state=42).fit(X_train.loc[:,ensem_clf_features],Y_train)\n",
    "eli5.show_weights(perm, feature_names = ensem_clf_features, top=None)\n",
    "df_feat_imp = pd.DataFrame(\n",
    "    data=np.concatenate((np.array(perm.feature_importances_)[:,np.newaxis],np.array(perm.feature_importances_std_)[:,np.newaxis]),axis=1),\n",
    "    index=ensem_clf_features,\n",
    "    columns=['feature importance','std'])\n",
    "df_feat_imp.sort_values(by=['feature importance'],axis='index', ascending=False,inplace=True)\n",
    "\n",
    "######################## plot ######################\n",
    "fig = plt.figure(dpi=300,figsize=(6,10))\n",
    "x = df_feat_imp['feature importance'].values.squeeze()\n",
    "y = np.arange(df_feat_imp.shape[0])\n",
    "y_labels = df_feat_imp.index.tolist()\n",
    "x_std = df_feat_imp['std']\n",
    "\n",
    "plt.barh(y, x, align='center', alpha=0.8)\n",
    "# 绘制标准差的误差线\n",
    "plt.errorbar(x, y, xerr=x_std, fmt='none', color='red', capsize=3)\n",
    "plt.yticks(y, y_labels)\n",
    "plt.xlabel(\"Feature Permutation Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.tight_layout()\n",
    "fn = os.path.join(dir_result,'Ensemble Model','Feature Permutation Importance.png')\n",
    "plt.savefig(fn, bbox_inches='tight',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T09:47:07.198844Z",
     "start_time": "2023-08-28T09:47:07.053555Z"
    }
   },
   "outputs": [],
   "source": [
    "# save the ensemble model as pkl file\n",
    "filename = os.path.join(dir_vars,'ensem_clf.pkl')\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(ensem_clfLite, file)\n",
    "\n",
    "# save the standard scaler as pkl file\n",
    "filename = os.path.join(dir_vars,'StandardScaler_model_named_sc.pkl')\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(sc, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T09:47:07.268724Z",
     "start_time": "2023-08-28T09:47:07.201940Z"
    }
   },
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "\n",
    "def get_package_version(package_name):\n",
    "    try:\n",
    "        version = pkg_resources.get_distribution(package_name).version\n",
    "        return version\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        return None\n",
    "\n",
    "def save_package_versions(requirements_file, package_names):\n",
    "    with open(requirements_file, 'w') as file:\n",
    "        for package_name in package_names:\n",
    "            version = get_package_version(package_name)\n",
    "            if version:\n",
    "                file.write(f\"{package_name}=={version}\\n\")\n",
    "            else:\n",
    "                file.write(f\"# Package '{package_name}' not found\\n\")\n",
    "\n",
    "\n",
    "# save the requirements as requirements.txt\n",
    "package_names = ['numpy', 'pandas', 'scikit-learn']\n",
    "requirements_file = os.path.join(dir_vars,'requirements.txt')\n",
    "save_package_versions(requirements_file, package_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "755.333px",
    "left": "22px",
    "top": "217px",
    "width": "342.803px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
